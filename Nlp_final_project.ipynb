{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nlp-final-project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNhxHcvQiaA03QGGIOPNpHk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MugiwaraNoRushi/Nlp-Project-2022/blob/main/Nlp_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Copy"
      ],
      "metadata": {
        "id": "EKiTDgwipLto"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Dkb_KzbJrQSl"
      },
      "outputs": [],
      "source": [
        "# google drive imports\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9mGDpM_rWqz",
        "outputId": "64aca87d-8b54-45ff-b605-65589e628d9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R gdrive/MyDrive/Project-NLP ./\n",
        "# Follow github instructions to create this folder Project-NLP and save it in your drive !! "
      ],
      "metadata": {
        "id": "vvO08f6DrX_F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First time when you do this, it will say to restart the runtime, do it and it will start working! \n",
        "!pip install -r Project-NLP/requirements.txt"
      ],
      "metadata": {
        "id": "oeSPpEbDrtuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The runtime env is ready to go\n",
        "# we can run our models here \n",
        "# The gpus have a time limit of around 4 hours"
      ],
      "metadata": {
        "id": "r0tYD3uBrwZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from allennlp.nn.util import batched_index_select\n",
        "from allennlp.nn import util, Activation\n",
        "from allennlp.modules import FeedForward\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from transformers import BertTokenizer, BertPreTrainedModel, BertModel\n",
        "from transformers import AlbertTokenizer, AlbertPreTrainedModel, AlbertModel\n",
        "\n",
        "import os\n",
        "import json\n",
        "import logging"
      ],
      "metadata": {
        "id": "YAm83Hwysk1Y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Set import\n",
        "import copy\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "yY2IENAhqMkZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function import\n",
        "import argparse\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "JkwtD0ncqMr7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8Eji47u8qMuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "2Qc2eXpSlidJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger('root')"
      ],
      "metadata": {
        "id": "PmOk1rxRn4eR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForEntity(BertPreTrainedModel):\n",
        "    def __init__(self, config, num_ner_labels, head_hidden_dim=150, width_embedding_dim=150, max_span_length=8):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.hidden_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.width_embedding = nn.Embedding(max_span_length+1, width_embedding_dim)\n",
        "        \n",
        "        self.ner_classifier = nn.Sequential(\n",
        "            FeedForward(input_dim=config.hidden_size*2+width_embedding_dim, \n",
        "                        num_layers=2,\n",
        "                        hidden_dims=head_hidden_dim,\n",
        "                        activations=F.relu,\n",
        "                        dropout=0.2),\n",
        "            nn.Linear(head_hidden_dim, num_ner_labels)\n",
        "        )\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _get_span_embeddings(self, input_ids, spans, token_type_ids=None, attention_mask=None):\n",
        "        sequence_output, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        sequence_output = self.hidden_dropout(sequence_output)\n",
        "\n",
        "        \"\"\"\n",
        "        spans: [batch_size, num_spans, 3]; 0: left_ned, 1: right_end, 2: width\n",
        "        spans_mask: (batch_size, num_spans, )\n",
        "        \"\"\"\n",
        "        spans_start = spans[:, :, 0].view(spans.size(0), -1)\n",
        "        spans_start_embedding = batched_index_select(sequence_output, spans_start)\n",
        "        spans_end = spans[:, :, 1].view(spans.size(0), -1)\n",
        "        spans_end_embedding = batched_index_select(sequence_output, spans_end)\n",
        "\n",
        "        spans_width = spans[:, :, 2].view(spans.size(0), -1)\n",
        "        spans_width_embedding = self.width_embedding(spans_width)\n",
        "\n",
        "        # Concatenate embeddings of left/right points and the width embedding\n",
        "        spans_embedding = torch.cat((spans_start_embedding, spans_end_embedding, spans_width_embedding), dim=-1)\n",
        "        \"\"\"\n",
        "        spans_embedding: (batch_size, num_spans, hidden_size*2+embedding_dim)\n",
        "        \"\"\"\n",
        "        return spans_embedding\n",
        "\n",
        "    def forward(self, input_ids, spans, spans_mask, spans_ner_label=None, token_type_ids=None, attention_mask=None):\n",
        "        spans_embedding = self._get_span_embeddings(input_ids, spans, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "        ffnn_hidden = []\n",
        "        hidden = spans_embedding\n",
        "        for layer in self.ner_classifier:\n",
        "            hidden = layer(hidden)\n",
        "            ffnn_hidden.append(hidden)\n",
        "        logits = ffnn_hidden[-1]\n",
        "\n",
        "        if spans_ner_label is not None:\n",
        "            loss_fct = CrossEntropyLoss(reduction='sum')\n",
        "            if attention_mask is not None:\n",
        "                active_loss = spans_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, logits.shape[-1])\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, spans_ner_label.view(-1), torch.tensor(loss_fct.ignore_index).type_as(spans_ner_label)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, logits.shape[-1]), spans_ner_label.view(-1))\n",
        "            return loss, logits, spans_embedding\n",
        "        else:\n",
        "            return logits, spans_embedding, spans_embedding"
      ],
      "metadata": {
        "id": "JAyyev0mm_uU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EntityModel():\n",
        "\n",
        "    def __init__(self, args, num_ner_labels):\n",
        "        super().__init__()\n",
        "\n",
        "        bert_model_name = args.model\n",
        "        vocab_name = bert_model_name\n",
        "        \n",
        "        if args.bert_model_dir is not None:\n",
        "            bert_model_name = str(args.bert_model_dir) + '/'\n",
        "            # vocab_name = bert_model_name + 'vocab.txt'\n",
        "            vocab_name = bert_model_name\n",
        "           \n",
        "\n",
        "       \n",
        "        self.tokenizer = BertTokenizer.from_pretrained(vocab_name)\n",
        "        self.bert_model = BertForEntity.from_pretrained(bert_model_name, num_ner_labels=num_ner_labels, max_span_length=args.max_span_length)\n",
        "\n",
        "        self._model_device = 'cpu'\n",
        "        self.move_model_to_cuda()\n",
        "\n",
        "    def move_model_to_cuda(self):\n",
        "        if not torch.cuda.is_available():\n",
        "            logger.error('No CUDA found!')\n",
        "            exit(-1)\n",
        "        logger.info('Moving to CUDA...')\n",
        "        self._model_device = 'cuda'\n",
        "        self.bert_model.cuda()\n",
        "        logger.info('# GPUs = %d'%(torch.cuda.device_count()))\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            self.bert_model = torch.nn.DataParallel(self.bert_model)\n",
        "\n",
        "    def _get_input_tensors(self, tokens, spans, spans_ner_label):\n",
        "        start2idx = []\n",
        "        end2idx = []\n",
        "        \n",
        "        bert_tokens = []\n",
        "        bert_tokens.append(self.tokenizer.cls_token)\n",
        "        for token in tokens:\n",
        "            start2idx.append(len(bert_tokens))\n",
        "            sub_tokens = self.tokenizer.tokenize(token)\n",
        "            bert_tokens += sub_tokens\n",
        "            end2idx.append(len(bert_tokens)-1)\n",
        "        bert_tokens.append(self.tokenizer.sep_token)\n",
        "\n",
        "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(bert_tokens)\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "        bert_spans = [[start2idx[span[0]], end2idx[span[1]], span[2]] for span in spans]\n",
        "        bert_spans_tensor = torch.tensor([bert_spans])\n",
        "\n",
        "        spans_ner_label_tensor = torch.tensor([spans_ner_label])\n",
        "\n",
        "        return tokens_tensor, bert_spans_tensor, spans_ner_label_tensor\n",
        "\n",
        "    def _get_input_tensors_batch(self, samples_list, training=True):\n",
        "        tokens_tensor_list = []\n",
        "        bert_spans_tensor_list = []\n",
        "        spans_ner_label_tensor_list = []\n",
        "        sentence_length = []\n",
        "\n",
        "        max_tokens = 0\n",
        "        max_spans = 0\n",
        "        for sample in samples_list:\n",
        "            tokens = sample['tokens']\n",
        "            spans = sample['spans']\n",
        "            spans_ner_label = sample['spans_label']\n",
        "\n",
        "            tokens_tensor, bert_spans_tensor, spans_ner_label_tensor = self._get_input_tensors(tokens, spans, spans_ner_label)\n",
        "            tokens_tensor_list.append(tokens_tensor)\n",
        "            bert_spans_tensor_list.append(bert_spans_tensor)\n",
        "            spans_ner_label_tensor_list.append(spans_ner_label_tensor)\n",
        "            assert(bert_spans_tensor.shape[1] == spans_ner_label_tensor.shape[1])\n",
        "            if (tokens_tensor.shape[1] > max_tokens):\n",
        "                max_tokens = tokens_tensor.shape[1]\n",
        "            if (bert_spans_tensor.shape[1] > max_spans):\n",
        "                max_spans = bert_spans_tensor.shape[1]\n",
        "            sentence_length.append(sample['sent_length'])\n",
        "        sentence_length = torch.Tensor(sentence_length)\n",
        "\n",
        "        # apply padding and concatenate tensors\n",
        "        final_tokens_tensor = None\n",
        "        final_attention_mask = None\n",
        "        final_bert_spans_tensor = None\n",
        "        final_spans_ner_label_tensor = None\n",
        "        final_spans_mask_tensor = None\n",
        "        for tokens_tensor, bert_spans_tensor, spans_ner_label_tensor in zip(tokens_tensor_list, bert_spans_tensor_list, spans_ner_label_tensor_list):\n",
        "            # padding for tokens\n",
        "            num_tokens = tokens_tensor.shape[1]\n",
        "            tokens_pad_length = max_tokens - num_tokens\n",
        "            attention_tensor = torch.full([1,num_tokens], 1, dtype=torch.long)\n",
        "            if tokens_pad_length>0:\n",
        "                pad = torch.full([1,tokens_pad_length], self.tokenizer.pad_token_id, dtype=torch.long)\n",
        "                tokens_tensor = torch.cat((tokens_tensor, pad), dim=1)\n",
        "                attention_pad = torch.full([1,tokens_pad_length], 0, dtype=torch.long)\n",
        "                attention_tensor = torch.cat((attention_tensor, attention_pad), dim=1)\n",
        "\n",
        "            # padding for spans\n",
        "            num_spans = bert_spans_tensor.shape[1]\n",
        "            spans_pad_length = max_spans - num_spans\n",
        "            spans_mask_tensor = torch.full([1,num_spans], 1, dtype=torch.long)\n",
        "            if spans_pad_length>0:\n",
        "                pad = torch.full([1,spans_pad_length,bert_spans_tensor.shape[2]], 0, dtype=torch.long)\n",
        "                bert_spans_tensor = torch.cat((bert_spans_tensor, pad), dim=1)\n",
        "                mask_pad = torch.full([1,spans_pad_length], 0, dtype=torch.long)\n",
        "                spans_mask_tensor = torch.cat((spans_mask_tensor, mask_pad), dim=1)\n",
        "                spans_ner_label_tensor = torch.cat((spans_ner_label_tensor, mask_pad), dim=1)\n",
        "\n",
        "            # update final outputs\n",
        "            if final_tokens_tensor is None:\n",
        "                final_tokens_tensor = tokens_tensor\n",
        "                final_attention_mask = attention_tensor\n",
        "                final_bert_spans_tensor = bert_spans_tensor\n",
        "                final_spans_ner_label_tensor = spans_ner_label_tensor\n",
        "                final_spans_mask_tensor = spans_mask_tensor\n",
        "            else:\n",
        "                final_tokens_tensor = torch.cat((final_tokens_tensor,tokens_tensor), dim=0)\n",
        "                final_attention_mask = torch.cat((final_attention_mask, attention_tensor), dim=0)\n",
        "                final_bert_spans_tensor = torch.cat((final_bert_spans_tensor, bert_spans_tensor), dim=0)\n",
        "                final_spans_ner_label_tensor = torch.cat((final_spans_ner_label_tensor, spans_ner_label_tensor), dim=0)\n",
        "                final_spans_mask_tensor = torch.cat((final_spans_mask_tensor, spans_mask_tensor), dim=0)\n",
        "        #logger.info(final_tokens_tensor)\n",
        "        #logger.info(final_attention_mask)\n",
        "        #logger.info(final_bert_spans_tensor)\n",
        "        #logger.info(final_bert_spans_tensor.shape)\n",
        "        #logger.info(final_spans_mask_tensor.shape)\n",
        "        #logger.info(final_spans_ner_label_tensor.shape)\n",
        "        return final_tokens_tensor, final_attention_mask, final_bert_spans_tensor, final_spans_mask_tensor, final_spans_ner_label_tensor, sentence_length\n",
        "\n",
        "    def run_batch(self, samples_list, try_cuda=True, training=True):\n",
        "        # convert samples to input tensors\n",
        "        tokens_tensor, attention_mask_tensor, bert_spans_tensor, spans_mask_tensor, spans_ner_label_tensor, sentence_length = self._get_input_tensors_batch(samples_list, training)\n",
        "\n",
        "        output_dict = {\n",
        "            'ner_loss': 0,\n",
        "        }\n",
        "\n",
        "        if training:\n",
        "            self.bert_model.train()\n",
        "            ner_loss, ner_logits, spans_embedding = self.bert_model(\n",
        "                input_ids = tokens_tensor.to(self._model_device),\n",
        "                spans = bert_spans_tensor.to(self._model_device),\n",
        "                spans_mask = spans_mask_tensor.to(self._model_device),\n",
        "                spans_ner_label = spans_ner_label_tensor.to(self._model_device),\n",
        "                attention_mask = attention_mask_tensor.to(self._model_device),\n",
        "            )\n",
        "            output_dict['ner_loss'] = ner_loss.sum()\n",
        "            output_dict['ner_llh'] = F.log_softmax(ner_logits, dim=-1)\n",
        "        else:\n",
        "            self.bert_model.eval()\n",
        "            with torch.no_grad():\n",
        "                ner_logits, spans_embedding, last_hidden = self.bert_model(\n",
        "                    input_ids = tokens_tensor.to(self._model_device),\n",
        "                    spans = bert_spans_tensor.to(self._model_device),\n",
        "                    spans_mask = spans_mask_tensor.to(self._model_device),\n",
        "                    spans_ner_label = None,\n",
        "                    attention_mask = attention_mask_tensor.to(self._model_device),\n",
        "                )\n",
        "            _, predicted_label = ner_logits.max(2)\n",
        "            predicted_label = predicted_label.cpu().numpy()\n",
        "            last_hidden = last_hidden.cpu().numpy()\n",
        "            \n",
        "            predicted = []\n",
        "            pred_prob = []\n",
        "            hidden = []\n",
        "            for i, sample in enumerate(samples_list):\n",
        "                ner = []\n",
        "                prob = []\n",
        "                lh = []\n",
        "                for j in range(len(sample['spans'])):\n",
        "                    ner.append(predicted_label[i][j])\n",
        "                    # prob.append(F.softmax(ner_logits[i][j], dim=-1).cpu().numpy())\n",
        "                    prob.append(ner_logits[i][j].cpu().numpy())\n",
        "                    lh.append(last_hidden[i][j])\n",
        "                predicted.append(ner)\n",
        "                pred_prob.append(prob)\n",
        "                hidden.append(lh)\n",
        "            output_dict['pred_ner'] = predicted\n",
        "            output_dict['ner_probs'] = pred_prob\n",
        "            output_dict['ner_last_hidden'] = hidden\n",
        "\n",
        "        return output_dict"
      ],
      "metadata": {
        "id": "cyfgQlC2nK42"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nJEUARP8nRPo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils for Models"
      ],
      "metadata": {
        "id": "_UxZMvd5phj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(samples, batch_size):\n",
        "    \"\"\"\n",
        "    Batchfy samples with a batch size\n",
        "    \"\"\"\n",
        "    num_samples = len(samples)\n",
        "\n",
        "    list_samples_batches = []\n",
        "    \n",
        "    # if a sentence is too long, make itself a batch to avoid GPU OOM\n",
        "    to_single_batch = []\n",
        "    for i in range(0, len(samples)):\n",
        "        if len(samples[i]['tokens']) > 350:\n",
        "            to_single_batch.append(i)\n",
        "    \n",
        "    for i in to_single_batch:\n",
        "        logger.info('Single batch sample: %s-%d', samples[i]['doc_key'], samples[i]['sentence_ix'])\n",
        "        list_samples_batches.append([samples[i]])\n",
        "    samples = [sample for i, sample in enumerate(samples) if i not in to_single_batch]\n",
        "\n",
        "    for i in range(0, len(samples), batch_size):\n",
        "        list_samples_batches.append(samples[i:i+batch_size])\n",
        "\n",
        "    assert(sum([len(batch) for batch in list_samples_batches]) == num_samples)\n",
        "\n",
        "    return list_samples_batches\n",
        "\n",
        "def overlap(s1, s2):\n",
        "    if s2.start_sent >= s1.start_sent and s2.start_sent <= s1.end_sent:\n",
        "        return True\n",
        "    if s2.end_sent >= s1.start_sent and s2.end_sent <= s1.end_sent:\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "4n_S6qWGoEhH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_dataset_to_samples(dataset, max_span_length, ner_label2id=None, context_window=0, split=0):\n",
        "    \"\"\"\n",
        "    Extract sentences and gold entities from a dataset\n",
        "    \"\"\"\n",
        "    # split: split the data into train and dev (for ACE04)\n",
        "    # split == 0: don't split\n",
        "    # split == 1: return first 90% (train)\n",
        "    # split == 2: return last 10% (dev)\n",
        "    samples = []\n",
        "    num_ner = 0\n",
        "    max_len = 0\n",
        "    max_ner = 0\n",
        "    num_overlap = 0\n",
        "    \n",
        "    if split == 0:\n",
        "        data_range = (0, len(dataset))\n",
        "    elif split == 1:\n",
        "        data_range = (0, int(len(dataset)*0.9))\n",
        "    elif split == 2:\n",
        "        data_range = (int(len(dataset)*0.9), len(dataset))\n",
        "\n",
        "    for c, doc in enumerate(dataset):\n",
        "        if c < data_range[0] or c >= data_range[1]:\n",
        "            continue\n",
        "        for i, sent in enumerate(doc):\n",
        "            num_ner += len(sent.ner)\n",
        "            sample = {\n",
        "                'doc_key': doc._doc_key,\n",
        "                'sentence_ix': sent.sentence_ix,\n",
        "            }\n",
        "            if context_window != 0 and len(sent.text) > context_window:\n",
        "                logger.info('Long sentence: {} {}'.format(sample, len(sent.text)))\n",
        "                # print('Exclude:', sample)\n",
        "                # continue\n",
        "            sample['tokens'] = sent.text\n",
        "            sample['sent_length'] = len(sent.text)\n",
        "            sent_start = 0\n",
        "            sent_end = len(sample['tokens'])\n",
        "\n",
        "            max_len = max(max_len, len(sent.text))\n",
        "            max_ner = max(max_ner, len(sent.ner))\n",
        "\n",
        "            if context_window > 0:\n",
        "                add_left = (context_window-len(sent.text)) // 2\n",
        "                add_right = (context_window-len(sent.text)) - add_left\n",
        "                \n",
        "                # add left context\n",
        "                j = i - 1\n",
        "                while j >= 0 and add_left > 0:\n",
        "                    context_to_add = doc[j].text[-add_left:]\n",
        "                    sample['tokens'] = context_to_add + sample['tokens']\n",
        "                    add_left -= len(context_to_add)\n",
        "                    sent_start += len(context_to_add)\n",
        "                    sent_end += len(context_to_add)\n",
        "                    j -= 1\n",
        "\n",
        "                # add right context\n",
        "                j = i + 1\n",
        "                while j < len(doc) and add_right > 0:\n",
        "                    context_to_add = doc[j].text[:add_right]\n",
        "                    sample['tokens'] = sample['tokens'] + context_to_add\n",
        "                    add_right -= len(context_to_add)\n",
        "                    j += 1\n",
        "\n",
        "            sample['sent_start'] = sent_start\n",
        "            sample['sent_end'] = sent_end\n",
        "            sample['sent_start_in_doc'] = sent.sentence_start\n",
        "            \n",
        "            sent_ner = {}\n",
        "            for ner in sent.ner:\n",
        "                sent_ner[ner.span.span_sent] = ner.label\n",
        "\n",
        "            span2id = {}\n",
        "            sample['spans'] = []\n",
        "            sample['spans_label'] = []\n",
        "            for i in range(len(sent.text)):\n",
        "                for j in range(i, min(len(sent.text), i+max_span_length)):\n",
        "                    sample['spans'].append((i+sent_start, j+sent_start, j-i+1))\n",
        "                    span2id[(i, j)] = len(sample['spans'])-1\n",
        "                    if (i, j) not in sent_ner:\n",
        "                        sample['spans_label'].append(0)\n",
        "                    else:\n",
        "                        sample['spans_label'].append(ner_label2id[sent_ner[(i, j)]])\n",
        "            samples.append(sample)\n",
        "    avg_length = sum([len(sample['tokens']) for sample in samples]) / len(samples)\n",
        "    max_length = max([len(sample['tokens']) for sample in samples])\n",
        "    logger.info('# Overlap: %d'%num_overlap)\n",
        "    logger.info('Extracted %d samples from %d documents, with %d NER labels, %.3f avg input length, %d max length'%(len(samples), data_range[1]-data_range[0], num_ner, avg_length, max_length))\n",
        "    logger.info('Max Length: %d, max NER: %d'%(max_len, max_ner))\n",
        "    return samples, num_ner"
      ],
      "metadata": {
        "id": "3Sx9-zJ3pjoJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NpEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        else:\n",
        "            return super(NpEncoder, self).default(obj)"
      ],
      "metadata": {
        "id": "TedFt5--pjq9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Set Class"
      ],
      "metadata": {
        "id": "yYeAc_5-oJOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fields_to_batches(d, keys_to_ignore=[]):\n",
        "    keys = [key for key in d.keys() if key not in keys_to_ignore]\n",
        "    lengths = [len(d[k]) for k in keys]\n",
        "    assert len(set(lengths)) == 1\n",
        "    length = lengths[0]\n",
        "    res = [{k: d[k][i] for k in keys} for i in range(length)]\n",
        "    return res\n",
        "\n",
        "def get_sentence_of_span(span, sentence_starts, doc_tokens):\n",
        "    \"\"\"\n",
        "    Return the index of the sentence that the span is part of.\n",
        "    \"\"\"\n",
        "    # Inclusive sentence ends\n",
        "    sentence_ends = [x - 1 for x in sentence_starts[1:]] + [doc_tokens - 1]\n",
        "    in_between = [span[0] >= start and span[1] <= end\n",
        "                  for start, end in zip(sentence_starts, sentence_ends)]\n",
        "    assert sum(in_between) == 1\n",
        "    the_sentence = in_between.index(True)\n",
        "    return the_sentence\n"
      ],
      "metadata": {
        "id": "UI9p9yfFoEj6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    def __init__(self, json_file, pred_file=None, doc_range=None):\n",
        "        self.js = self._read(json_file, pred_file)\n",
        "        if doc_range is not None:\n",
        "            self.js = self.js[doc_range[0]:doc_range[1]]\n",
        "        self.documents = [Document(js) for js in self.js]\n",
        "\n",
        "    def update_from_js(self, js):\n",
        "        self.js = js\n",
        "        self.documents = [Document(js) for js in self.js]\n",
        "\n",
        "    def _read(self, json_file, pred_file=None):\n",
        "        gold_docs = [json.loads(line) for line in open(json_file)]\n",
        "        if pred_file is None:\n",
        "            return gold_docs\n",
        "\n",
        "        pred_docs = [json.loads(line) for line in open(pred_file)]\n",
        "        merged_docs = []\n",
        "        for gold, pred in zip(gold_docs, pred_docs):\n",
        "            assert gold[\"doc_key\"] == pred[\"doc_key\"]\n",
        "            assert gold[\"sentences\"] == pred[\"sentences\"]\n",
        "            merged = copy.deepcopy(gold)\n",
        "            for k, v in pred.items():\n",
        "                if \"predicted\" in k:\n",
        "                    merged[k] = v\n",
        "            merged_docs.append(merged)\n",
        "\n",
        "        return merged_docs\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        return self.documents[ix]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "####################\n",
        "\n",
        "# Code to do evaluation of predictions for a loaded dataset.\n",
        "\n",
        "def safe_div(num, denom):\n",
        "    if denom > 0:\n",
        "        return num / denom\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def compute_f1(predicted, gold, matched):\n",
        "    # F1 score.\n",
        "    precision = safe_div(matched, predicted)\n",
        "    recall = safe_div(matched, gold)\n",
        "    f1 = safe_div(2 * precision * recall, precision + recall)\n",
        "    return dict(precision=precision, recall=recall, f1=f1)\n",
        "\n",
        "\n",
        "def evaluate_sent(sent, counts):\n",
        "    correct_ner = set()\n",
        "    # Entities.\n",
        "    counts[\"ner_gold\"] += len(sent.ner)\n",
        "    counts[\"ner_predicted\"] += len(sent.predicted_ner)\n",
        "    for prediction in sent.predicted_ner:\n",
        "        if any([prediction == actual for actual in sent.ner]):\n",
        "            counts[\"ner_matched\"] += 1\n",
        "            correct_ner.add(prediction.span)\n",
        "\n",
        "    # Relations.\n",
        "    counts[\"relations_gold\"] += len(sent.relations)\n",
        "    counts[\"relations_predicted\"] += len(sent.predicted_relations)\n",
        "    for prediction in sent.predicted_relations:\n",
        "        if any([prediction == actual for actual in sent.relations]):\n",
        "            counts[\"relations_matched\"] += 1\n",
        "            if (prediction.pair[0] in correct_ner) and (prediction.pair[1] in correct_ner):\n",
        "                counts[\"strict_relations_matched\"] += 1\n",
        "\n",
        "    # Return the updated counts.\n",
        "    return counts\n",
        "\n",
        "def evaluate_predictions(dataset):\n",
        "    counts = Counter()\n",
        "\n",
        "    for doc in dataset:\n",
        "        for sent in doc:\n",
        "            counts = evaluate_sent(sent, counts)\n",
        "\n",
        "    scores_ner = compute_f1(\n",
        "        counts[\"ner_predicted\"], counts[\"ner_gold\"], counts[\"ner_matched\"])\n",
        "    scores_relations = compute_f1(\n",
        "        counts[\"relations_predicted\"], counts[\"relations_gold\"], counts[\"relations_matched\"])\n",
        "    scores_strict_relations = compute_f1(\n",
        "        counts[\"relations_predicted\"], counts[\"relations_gold\"], counts[\"strict_relations_matched\"])\n",
        "\n",
        "    return dict(ner=scores_ner, relation=scores_relations, strict_relation=scores_strict_relations)\n",
        "\n",
        "def analyze_relation_coverage(dataset):\n",
        "    \n",
        "    def overlap(s1, s2):\n",
        "        if s2.start_sent >= s1.start_sent and s2.start_sent <= s1.end_sent:\n",
        "            return True\n",
        "        if s2.end_sent >= s1.start_sent and s2.end_sent <= s1.end_sent:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    nrel_gold = 0\n",
        "    nrel_pred_cover = 0\n",
        "    nrel_top_cover = 0\n",
        "\n",
        "    npair_pred = 0\n",
        "    npair_top = 0\n",
        "\n",
        "    nrel_overlap = 0\n",
        "\n",
        "    for d in dataset:\n",
        "        for s in d:\n",
        "            pred = set([ner.span for ner in s.predicted_ner])\n",
        "            top = set([ner.span for ner in s.top_spans])\n",
        "            npair_pred += len(s.predicted_ner) * (len(s.predicted_ner) - 1)\n",
        "            npair_top += len(s.top_spans) * (len(s.top_spans) - 1)\n",
        "            for r in s.relations:\n",
        "                nrel_gold += 1\n",
        "                if (r.pair[0] in pred) and (r.pair[1] in pred):\n",
        "                    nrel_pred_cover += 1\n",
        "                if (r.pair[0] in top) and (r.pair[1] in top):\n",
        "                    nrel_top_cover += 1\n",
        "                \n",
        "                if overlap(r.pair[0], r.pair[1]):\n",
        "                    nrel_overlap += 1\n",
        "\n",
        "    print('Coverage by predicted entities: %.3f (%d / %d), #candidates: %d'%(nrel_pred_cover/nrel_gold*100.0, nrel_pred_cover, nrel_gold, npair_pred))\n",
        "    print('Coverage by top 0.4 spans: %.3f (%d / %d), #candidates: %d'%(nrel_top_cover/nrel_gold*100.0, nrel_top_cover, nrel_gold, npair_top))\n",
        "    print('Overlap: %.3f (%d / %d)'%(nrel_overlap / nrel_gold * 100.0, nrel_overlap, nrel_gold))\n"
      ],
      "metadata": {
        "id": "_wRt2WCtoEnD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Document:\n",
        "    def __init__(self, js):\n",
        "        self._doc_key = js[\"doc_key\"]\n",
        "        entries = fields_to_batches(js, [\"doc_key\", \"clusters\", \"predicted_clusters\", \"section_starts\"])\n",
        "        sentence_lengths = [len(entry[\"sentences\"]) for entry in entries]\n",
        "        sentence_starts = np.cumsum(sentence_lengths)\n",
        "        sentence_starts = np.roll(sentence_starts, 1)\n",
        "        sentence_starts[0] = 0\n",
        "        self.sentence_starts = sentence_starts\n",
        "        self.sentences = [Sentence(entry, sentence_start, sentence_ix)\n",
        "                          for sentence_ix, (entry, sentence_start)\n",
        "                          in enumerate(zip(entries, sentence_starts))]\n",
        "        if \"clusters\" in js:\n",
        "            self.clusters = [Cluster(entry, i, self)\n",
        "                             for i, entry in enumerate(js[\"clusters\"])]\n",
        "        if \"predicted_clusters\" in js:\n",
        "            self.predicted_clusters = [Cluster(entry, i, self)\n",
        "                                       for i, entry in enumerate(js[\"predicted_clusters\"])]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"\\n\".join([str(i) + \": \" + \" \".join(sent.text) for i, sent in enumerate(self.sentences)])\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        return self.sentences[ix]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def print_plaintext(self):\n",
        "        for sent in self:\n",
        "            print(\" \".join(sent.text))\n",
        "\n",
        "\n",
        "    def find_cluster(self, entity, predicted=True):\n",
        "        \"\"\"\n",
        "        Search through erence clusters and return the one containing the query entity, if it's\n",
        "        part of a cluster. If we don't find a match, return None.\n",
        "        \"\"\"\n",
        "        clusters = self.predicted_clusters if predicted else self.clusters\n",
        "        for clust in clusters:\n",
        "            for entry in clust:\n",
        "                if entry.span == entity.span:\n",
        "                    return clust\n",
        "\n",
        "        return None\n",
        "\n",
        "    @property\n",
        "    def n_tokens(self):\n",
        "        return sum([len(sent) for sent in self.sentences])\n",
        "\n",
        "\n",
        "class Sentence:\n",
        "    def __init__(self, entry, sentence_start, sentence_ix):\n",
        "        self.sentence_start = sentence_start\n",
        "        self.text = entry[\"sentences\"]\n",
        "        self.sentence_ix = sentence_ix\n",
        "        # Gold\n",
        "        if \"ner_flavor\" in entry:\n",
        "            self.ner = [NER(this_ner, self.text, sentence_start, flavor=this_flavor)\n",
        "                        for this_ner, this_flavor in zip(entry[\"ner\"], entry[\"ner_flavor\"])]\n",
        "        elif \"ner\" in entry:\n",
        "            self.ner = [NER(this_ner, self.text, sentence_start)\n",
        "                        for this_ner in entry[\"ner\"]]\n",
        "        if \"relations\" in entry:\n",
        "            self.relations = [Relation(this_relation, self.text, sentence_start) for\n",
        "                              this_relation in entry[\"relations\"]]\n",
        "        if \"events\" in entry:\n",
        "            self.events = Events(entry[\"events\"], self.text, sentence_start)\n",
        "\n",
        "        # Predicted\n",
        "        if \"predicted_ner\" in entry:\n",
        "            self.predicted_ner = [NER(this_ner, self.text, sentence_start, flavor=None) for\n",
        "                                  this_ner in entry[\"predicted_ner\"]]\n",
        "        if \"predicted_relations\" in entry:\n",
        "            self.predicted_relations = [Relation(this_relation, self.text, sentence_start) for\n",
        "                                        this_relation in entry[\"predicted_relations\"]]\n",
        "        if \"predicted_events\" in entry:\n",
        "            self.predicted_events = Events(entry[\"predicted_events\"], self.text, sentence_start)\n",
        "\n",
        "        # Top spans\n",
        "        if \"top_spans\" in entry:\n",
        "            self.top_spans = [NER(this_ner, self.text, sentence_start, flavor=None) for\n",
        "                                this_ner in entry[\"top_spans\"]]\n",
        "\n",
        "    def __repr__(self):\n",
        "        the_text = \" \".join(self.text)\n",
        "        the_lengths = np.array([len(x) for x in self.text])\n",
        "        tok_ixs = \"\"\n",
        "        for i, offset in enumerate(the_lengths):\n",
        "            true_offset = offset if i < 10 else offset - 1\n",
        "            tok_ixs += str(i)\n",
        "            tok_ixs += \" \" * true_offset\n",
        "\n",
        "        return the_text + \"\\n\" + tok_ixs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def get_flavor(self, argument):\n",
        "        the_ner = [x for x in self.ner if x.span == argument.span]\n",
        "        if len(the_ner) > 1:\n",
        "            print(\"Weird\")\n",
        "        if the_ner:\n",
        "            the_flavor = the_ner[0].flavor\n",
        "        else:\n",
        "            the_flavor = None\n",
        "        return the_flavor\n",
        "\n",
        "\n",
        "class Span:\n",
        "    def __init__(self, start, end, text, sentence_start):\n",
        "        self.start_doc = start\n",
        "        self.end_doc = end\n",
        "        self.span_doc = (self.start_doc, self.end_doc)\n",
        "        self.start_sent = start - sentence_start\n",
        "        self.end_sent = end - sentence_start\n",
        "        self.span_sent = (self.start_sent, self.end_sent)\n",
        "        self.text = text[self.start_sent:self.end_sent + 1]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str((self.start_sent, self.end_sent, self.text))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return (self.span_doc == other.span_doc and\n",
        "                self.span_sent == other.span_sent and\n",
        "                self.text == other.text)\n",
        "\n",
        "    def __hash__(self):\n",
        "        tup = self.span_doc + self.span_sent + (\" \".join(self.text),)\n",
        "        return hash(tup)\n",
        "\n",
        "\n",
        "class Token:\n",
        "    def __init__(self, ix, text, sentence_start):\n",
        "        self.ix_doc = ix\n",
        "        self.ix_sent = ix - sentence_start\n",
        "        self.text = text[self.ix_sent]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str((self.ix_sent, self.text))\n",
        "\n",
        "\n",
        "class Trigger:\n",
        "    def __init__(self, token, label):\n",
        "        self.token = token\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.token.__repr__()[:-1] + \", \" + self.label + \")\"\n",
        "\n",
        "\n",
        "class Argument:\n",
        "    def __init__(self, span, role, event_type):\n",
        "        self.span = span\n",
        "        self.role = role\n",
        "        self.event_type = event_type\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.span.__repr__()[:-1] + \", \" + self.event_type + \", \" + self.role + \")\"\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return (self.span == other.span and\n",
        "                self.role == other.role and\n",
        "                self.event_type == other.event_type)\n",
        "\n",
        "    def __hash__(self):\n",
        "        return self.span.__hash__() + hash((self.role, self.event_type))\n",
        "\n",
        "\n",
        "class NER:\n",
        "    def __init__(self, ner, text, sentence_start, flavor=None):\n",
        "        self.span = Span(ner[0], ner[1], text, sentence_start)\n",
        "        self.label = ner[2]\n",
        "        self.flavor = flavor\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.span.__repr__() + \": \" + self.label\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return (self.span == other.span and\n",
        "                self.label == other.label and\n",
        "                self.flavor == other.flavor)\n",
        "\n",
        "\n",
        "class Relation:\n",
        "    def __init__(self, relation, text, sentence_start):\n",
        "        start1, end1 = relation[0], relation[1]\n",
        "        start2, end2 = relation[2], relation[3]\n",
        "        label = relation[4]\n",
        "        span1 = Span(start1, end1, text, sentence_start)\n",
        "        span2 = Span(start2, end2, text, sentence_start)\n",
        "        self.pair = (span1, span2)\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.pair[0].__repr__() + \", \" + self.pair[1].__repr__() + \": \" + self.label\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return (self.pair == other.pair) and (self.label == other.label)\n",
        "\n",
        "\n",
        "class AtomicRelation:\n",
        "    def __init__(self, ent0, ent1, label):\n",
        "        self.ent0 = ent0\n",
        "        self.ent1 = ent1\n",
        "        self.label = label\n",
        "\n",
        "    @classmethod\n",
        "    def from_relation(cls, relation):\n",
        "        ent0 = \" \".join(relation.pair[0].text)\n",
        "        ent1 = \" \".join(relation.pair[1].text)\n",
        "        label = relation.label\n",
        "        return cls(ent0, ent1, label)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"({self.ent0} | {self.ent1} | {self.label})\"\n",
        "\n",
        "\n",
        "\n",
        "class Event:\n",
        "    def __init__(self, event, text, sentence_start):\n",
        "        trig = event[0]\n",
        "        args = event[1:]\n",
        "        trigger_token = Token(trig[0], text, sentence_start)\n",
        "        self.trigger = Trigger(trigger_token, trig[1])\n",
        "\n",
        "        self.arguments = []\n",
        "        for arg in args:\n",
        "            span = Span(arg[0], arg[1], text, sentence_start)\n",
        "            self.arguments.append(Argument(span, arg[2], self.trigger.label))\n",
        "\n",
        "    def __repr__(self):\n",
        "        res = \"<\"\n",
        "        res += self.trigger.__repr__() + \":\\n\"\n",
        "        for arg in self.arguments:\n",
        "            res += 6 * \" \" + arg.__repr__() + \";\\n\"\n",
        "        res = res[:-2] + \">\"\n",
        "        return res\n",
        "\n",
        "\n",
        "class Events:\n",
        "    def __init__(self, events_json, text, sentence_start):\n",
        "        self.event_list = [Event(this_event, text, sentence_start) for this_event in events_json]\n",
        "        self.triggers = set([event.trigger for event in self.event_list])\n",
        "        self.arguments = set([arg for event in self.event_list for arg in event.arguments])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.event_list)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "       return self.event_list[i]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"\\n\\n\".join([event.__repr__() for event in self.event_list])\n",
        "\n",
        "    def span_matches(self, argument):\n",
        "        return set([candidate for candidate in self.arguments\n",
        "                    if candidate.span.span_sent == argument.span.span_sent])\n",
        "\n",
        "    def event_type_matches(self, argument):\n",
        "        return set([candidate for candidate in self.span_matches(argument)\n",
        "                    if candidate.event_type == argument.event_type])\n",
        "\n",
        "    def matches_except_event_type(self, argument):\n",
        "        matched = [candidate for candidate in self.span_matches(argument)\n",
        "                   if candidate.event_type != argument.event_type\n",
        "                   and candidate.role == argument.role]\n",
        "        return set(matched)\n",
        "\n",
        "    def exact_match(self, argument):\n",
        "        for candidate in self.arguments:\n",
        "            if candidate == argument:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "class Cluster:\n",
        "    def __init__(self, cluster, cluster_id, document):\n",
        "        members = []\n",
        "        for entry in cluster:\n",
        "            sentence_ix = get_sentence_of_span(entry, document.sentence_starts, document.n_tokens)\n",
        "            sentence = document[sentence_ix]\n",
        "            span = Span(entry[0], entry[1], sentence.text, sentence.sentence_start)\n",
        "            ners = [x for x in sentence.ner if x.span == span]\n",
        "            assert len(ners) <= 1\n",
        "            ner = ners[0] if len(ners) == 1 else None\n",
        "            to_append = ClusterMember(span, ner, sentence, cluster_id)\n",
        "            members.append(to_append)\n",
        "\n",
        "        self.members = members\n",
        "        self.cluster_id = cluster_id\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.cluster_id}: \" + self.members.__repr__()\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        return self.members[ix]\n",
        "\n",
        "\n",
        "class ClusterMember:\n",
        "    def __init__(self, span, ner, sentence, cluster_id):\n",
        "        self.span = span\n",
        "        self.ner = ner\n",
        "        self.sentence = sentence\n",
        "        self.cluster_id = cluster_id\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<{self.sentence.sentence_ix}> \" + self.span.__repr__()\n"
      ],
      "metadata": {
        "id": "G90kJsNHo-Uw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ycm9eD8ypAuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils for Data Sets "
      ],
      "metadata": {
        "id": "aIKWEqn-pa7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task_ner_labels = {\n",
        "    'ace04': ['FAC', 'WEA', 'LOC', 'VEH', 'GPE', 'ORG', 'PER'],\n",
        "    'ace05': ['FAC', 'WEA', 'LOC', 'VEH', 'GPE', 'ORG', 'PER'],\n",
        "    'scierc': ['Method', 'OtherScientificTerm', 'Task', 'Generic', 'Material', 'Metric'],\n",
        "}\n",
        "\n",
        "def get_labelmap(label_list):\n",
        "    label2id = {}\n",
        "    id2label = {}\n",
        "    for i, label in enumerate(label_list):\n",
        "        label2id[label] = i + 1\n",
        "        id2label[i + 1] = label\n",
        "    return label2id, id2label\n"
      ],
      "metadata": {
        "id": "hQzGjw0updfR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main function"
      ],
      "metadata": {
        "id": "LlFGSK19pFLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger('root')"
      ],
      "metadata": {
        "id": "LgdE5NmauzPT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, args):\n",
        "    \"\"\"\n",
        "    Save the model to the output directory\n",
        "    \"\"\"\n",
        "    logger.info('Saving model to %s...'%(args.output_dir))\n",
        "    model_to_save = model.bert_model.module if hasattr(model.bert_model, 'module') else model.bert_model\n",
        "    model_to_save.save_pretrained(args.output_dir)\n",
        "    model.tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "def output_ner_predictions(model, batches, dataset, output_file):\n",
        "    \"\"\"\n",
        "    Save the prediction as a json file\n",
        "    \"\"\"\n",
        "    ner_result = {}\n",
        "    span_hidden_table = {}\n",
        "    tot_pred_ett = 0\n",
        "    for i in range(len(batches)):\n",
        "        output_dict = model.run_batch(batches[i], training=False)\n",
        "        pred_ner = output_dict['pred_ner']\n",
        "        for sample, preds in zip(batches[i], pred_ner):\n",
        "            off = sample['sent_start_in_doc'] - sample['sent_start']\n",
        "            k = sample['doc_key'] + '-' + str(sample['sentence_ix'])\n",
        "            ner_result[k] = []\n",
        "            for span, pred in zip(sample['spans'], preds):\n",
        "                span_id = '%s::%d::(%d,%d)'%(sample['doc_key'], sample['sentence_ix'], span[0]+off, span[1]+off)\n",
        "                if pred == 0:\n",
        "                    continue\n",
        "                ner_result[k].append([span[0]+off, span[1]+off, ner_id2label[pred]])\n",
        "            tot_pred_ett += len(ner_result[k])\n",
        "\n",
        "    logger.info('Total pred entities: %d'%tot_pred_ett)\n",
        "\n",
        "    js = dataset.js\n",
        "    for i, doc in enumerate(js):\n",
        "        doc[\"predicted_ner\"] = []\n",
        "        doc[\"predicted_relations\"] = []\n",
        "        for j in range(len(doc[\"sentences\"])):\n",
        "            k = doc['doc_key'] + '-' + str(j)\n",
        "            if k in ner_result:\n",
        "                doc[\"predicted_ner\"].append(ner_result[k])\n",
        "            else:\n",
        "                logger.info('%s not in NER results!'%k)\n",
        "                doc[\"predicted_ner\"].append([])\n",
        "            \n",
        "            doc[\"predicted_relations\"].append([])\n",
        "\n",
        "        js[i] = doc\n",
        "\n",
        "    logger.info('Output predictions to %s..'%(output_file))\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write('\\n'.join(json.dumps(doc, cls=NpEncoder) for doc in js))\n",
        "\n",
        "def evaluate(model, batches, tot_gold):\n",
        "    \"\"\"\n",
        "    Evaluate the entity model\n",
        "    \"\"\"\n",
        "    logger.info('Evaluating...')\n",
        "    c_time = time.time()\n",
        "    cor = 0\n",
        "    tot_pred = 0\n",
        "    l_cor = 0\n",
        "    l_tot = 0\n",
        "\n",
        "    for i in range(len(batches)):\n",
        "        output_dict = model.run_batch(batches[i], training=False)\n",
        "        pred_ner = output_dict['pred_ner']\n",
        "        for sample, preds in zip(batches[i], pred_ner):\n",
        "            for gold, pred in zip(sample['spans_label'], preds):\n",
        "                l_tot += 1\n",
        "                if pred == gold:\n",
        "                    l_cor += 1\n",
        "                if pred != 0 and gold != 0 and pred == gold:\n",
        "                    cor += 1\n",
        "                if pred != 0:\n",
        "                    tot_pred += 1\n",
        "                   \n",
        "    acc = l_cor / l_tot\n",
        "    logger.info('Accuracy: %5f'%acc)\n",
        "    logger.info('Cor: %d, Pred TOT: %d, Gold TOT: %d'%(cor, tot_pred, tot_gold))\n",
        "    p = cor / tot_pred if cor > 0 else 0.0\n",
        "    r = cor / tot_gold if cor > 0 else 0.0\n",
        "    f1 = 2 * (p * r) / (p + r) if cor > 0 else 0.0\n",
        "    logger.info('P: %.5f, R: %.5f, F1: %.5f'%(p, r, f1))\n",
        "    logger.info('Used time: %f'%(time.time()-c_time))\n",
        "    return f1\n",
        "\n",
        "def setseed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n"
      ],
      "metadata": {
        "id": "J-vXNnWkpG3z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--task', type=str, default=None, required=True, choices=['ace04', 'ace05', 'scierc'])\n",
        "\n",
        "parser.add_argument('--data_dir', type=str, default=None, required=True, \n",
        "                    help=\"path to the preprocessed dataset\")\n",
        "parser.add_argument('--output_dir', type=str, default='entity_output', \n",
        "                    help=\"output directory of the entity model\")\n",
        "\n",
        "parser.add_argument('--max_span_length', type=int, default=8, \n",
        "                    help=\"spans w/ length up to max_span_length are considered as candidates\")\n",
        "parser.add_argument('--train_batch_size', type=int, default=32, \n",
        "                    help=\"batch size during training\")\n",
        "parser.add_argument('--eval_batch_size', type=int, default=32, \n",
        "                    help=\"batch size during inference\")\n",
        "parser.add_argument('--learning_rate', type=float, default=1e-5, \n",
        "                    help=\"learning rate for the BERT encoder\")\n",
        "parser.add_argument('--task_learning_rate', type=float, default=1e-4, \n",
        "                    help=\"learning rate for task-specific parameters, i.e., classification head\")\n",
        "parser.add_argument('--warmup_proportion', type=float, default=0.1, \n",
        "                    help=\"the ratio of the warmup steps to the total steps\")\n",
        "parser.add_argument('--num_epoch', type=int, default=100, \n",
        "                    help=\"number of the training epochs\")\n",
        "parser.add_argument('--print_loss_step', type=int, default=100, \n",
        "                    help=\"how often logging the loss value during training\")\n",
        "parser.add_argument('--eval_per_epoch', type=int, default=1, \n",
        "                    help=\"how often evaluating the trained model on dev set during training\")\n",
        "parser.add_argument(\"--bertadam\", action=\"store_true\", help=\"If bertadam, then set correct_bias = False\")\n",
        "\n",
        "parser.add_argument('--do_train', action='store_true', \n",
        "                    help=\"whether to run training\")\n",
        "parser.add_argument('--train_shuffle', action='store_true',\n",
        "                    help=\"whether to train with randomly shuffled data\")\n",
        "parser.add_argument('--do_eval', action='store_true', \n",
        "                    help=\"whether to run evaluation\")\n",
        "parser.add_argument('--eval_test', action='store_true', \n",
        "                    help=\"whether to evaluate on test set\")\n",
        "parser.add_argument('--dev_pred_filename', type=str, default=\"ent_pred_dev.json\", help=\"the prediction filename for the dev set\")\n",
        "parser.add_argument('--test_pred_filename', type=str, default=\"ent_pred_test.json\", help=\"the prediction filename for the test set\")\n",
        "\n",
        "parser.add_argument('--use_albert', action='store_true', \n",
        "                    help=\"whether to use ALBERT model\")\n",
        "parser.add_argument('--model', type=str, default='bert-base-uncased', \n",
        "                    help=\"the base model name (a huggingface model)\")\n",
        "parser.add_argument('--bert_model_dir', type=str, default=None, \n",
        "                    help=\"the base model directory\")\n",
        "\n",
        "parser.add_argument('--seed', type=int, default=0)\n",
        "\n",
        "parser.add_argument('--context_window', type=int, required=True, default=None, \n",
        "                    help=\"the context window size W for the entity model\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "args.train_data = os.path.join(args.data_dir, 'train.json')\n",
        "args.dev_data = os.path.join(args.data_dir, 'dev.json')\n",
        "args.test_data = os.path.join(args.data_dir, 'test.json')\n",
        "\n",
        "if 'albert' in args.model:\n",
        "    logger.info('Use Albert: %s'%args.model)\n",
        "    args.use_albert = True\n",
        "\n",
        "setseed(args.seed)\n",
        "\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)\n",
        "\n",
        "if args.do_train:\n",
        "    logger.addHandler(logging.FileHandler(os.path.join(args.output_dir, \"train.log\"), 'w'))\n",
        "else:\n",
        "    logger.addHandler(logging.FileHandler(os.path.join(args.output_dir, \"eval.log\"), 'w'))\n",
        "\n",
        "logger.info(sys.argv)\n",
        "logger.info(args)\n",
        "\n",
        "ner_label2id, ner_id2label = get_labelmap(task_ner_labels[args.task])\n",
        "\n",
        "num_ner_labels = len(task_ner_labels[args.task]) + 1\n",
        "model = EntityModel(args, num_ner_labels=num_ner_labels)\n",
        "\n",
        "dev_data = Dataset(args.dev_data)\n",
        "dev_samples, dev_ner = convert_dataset_to_samples(dev_data, args.max_span_length, ner_label2id=ner_label2id, context_window=args.context_window)\n",
        "dev_batches = batchify(dev_samples, args.eval_batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "_ztNFqvEu5jb",
        "outputId": "245113fe-40c0-4528-ad5c-555df03eb2e3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] --task {ace04,ace05,scierc} --data_dir\n",
            "                             DATA_DIR [--output_dir OUTPUT_DIR]\n",
            "                             [--max_span_length MAX_SPAN_LENGTH]\n",
            "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
            "                             [--eval_batch_size EVAL_BATCH_SIZE]\n",
            "                             [--learning_rate LEARNING_RATE]\n",
            "                             [--task_learning_rate TASK_LEARNING_RATE]\n",
            "                             [--warmup_proportion WARMUP_PROPORTION]\n",
            "                             [--num_epoch NUM_EPOCH]\n",
            "                             [--print_loss_step PRINT_LOSS_STEP]\n",
            "                             [--eval_per_epoch EVAL_PER_EPOCH] [--bertadam]\n",
            "                             [--do_train] [--train_shuffle] [--do_eval]\n",
            "                             [--eval_test]\n",
            "                             [--dev_pred_filename DEV_PRED_FILENAME]\n",
            "                             [--test_pred_filename TEST_PRED_FILENAME]\n",
            "                             [--use_albert] [--model MODEL]\n",
            "                             [--bert_model_dir BERT_MODEL_DIR] [--seed SEED]\n",
            "                             --context_window CONTEXT_WINDOW\n",
            "ipykernel_launcher.py: error: the following arguments are required: --task, --data_dir, --context_window\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.do_train:\n",
        "    train_data = Dataset(args.train_data)\n",
        "    train_samples, train_ner = convert_dataset_to_samples(train_data, args.max_span_length, ner_label2id=ner_label2id, context_window=args.context_window)\n",
        "    train_batches = batchify(train_samples, args.train_batch_size)\n",
        "    best_result = 0.0\n",
        "\n",
        "    param_optimizer = list(model.bert_model.named_parameters())\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer\n",
        "            if 'bert' in n]},\n",
        "        {'params': [p for n, p in param_optimizer\n",
        "            if 'bert' not in n], 'lr': args.task_learning_rate}]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, correct_bias=not(args.bertadam))\n",
        "    t_total = len(train_batches) * args.num_epoch\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, int(t_total*args.warmup_proportion), t_total)\n",
        "    \n",
        "    tr_loss = 0\n",
        "    tr_examples = 0\n",
        "    global_step = 0\n",
        "    eval_step = len(train_batches) // args.eval_per_epoch\n",
        "    for _ in tqdm(range(args.num_epoch)):\n",
        "        if args.train_shuffle:\n",
        "            random.shuffle(train_batches)\n",
        "        for i in tqdm(range(len(train_batches))):\n",
        "            output_dict = model.run_batch(train_batches[i], training=True)\n",
        "            loss = output_dict['ner_loss']\n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            tr_examples += len(train_batches[i])\n",
        "            global_step += 1\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if global_step % args.print_loss_step == 0:\n",
        "                logger.info('Epoch=%d, iter=%d, loss=%.5f'%(_, i, tr_loss / tr_examples))\n",
        "                tr_loss = 0\n",
        "                tr_examples = 0\n",
        "\n",
        "            if global_step % eval_step == 0:\n",
        "                f1 = evaluate(model, dev_batches, dev_ner)\n",
        "                if f1 > best_result:\n",
        "                    best_result = f1\n",
        "                    logger.info('!!! Best valid (epoch=%d): %.2f' % (_, f1*100))\n",
        "                    save_model(model, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "bwuoY0vZu8dp",
        "outputId": "6c655063-c65b-4e08-d4a1-a93f3f582b64"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-8cb1feab98c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_dataset_to_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_span_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mner_label2id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mner_label2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.do_eval:\n",
        "    args.bert_model_dir = args.output_dir\n",
        "    model = EntityModel(args, num_ner_labels=num_ner_labels)\n",
        "    if args.eval_test:\n",
        "        test_data = Dataset(args.test_data)\n",
        "        prediction_file = os.path.join(args.output_dir, args.test_pred_filename)\n",
        "    else:\n",
        "        test_data = Dataset(args.dev_data)\n",
        "        prediction_file = os.path.join(args.output_dir, args.dev_pred_filename)\n",
        "    test_samples, test_ner = convert_dataset_to_samples(test_data, args.max_span_length, ner_label2id=ner_label2id, context_window=args.context_window)\n",
        "    test_batches = batchify(test_samples, args.eval_batch_size)\n",
        "    evaluate(model, test_batches, test_ner)\n",
        "    output_ner_predictions(model, test_batches, test_data, output_file=prediction_file)"
      ],
      "metadata": {
        "id": "CITttB0nvIGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}